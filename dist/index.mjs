import{KafkaConsumer as k,Producer as C,AdminClient as w,CODES as l}from"@confluentinc/kafka-javascript";import{Buffer as h}from"buffer";import{Store as O}from"buckets";import{once as p}from"events";function b(n,t){return n.topics.filter(e=>t.some(r=>typeof r=="string"?e.name===r:r instanceof RegExp?r.test(e.name):!1))}function m(n,t){return new Promise((e,r)=>n.getMetadata(t,(s,o)=>s?r(s):e(o)))}function f(n,t){return b(n,t).flatMap(r=>r.partitions.map(s=>({topic:r.name,partition:s.id,offset:0})))}var d=class{constructor(t){this.initialized=!1;this.watermarks=new Map;this.topicOffsets=new Map;let{createTopicOptions:e,appId:r,brokers:s,store:o,consumer:a,producer:i}=t;this.topic=e?e.topic:`${r}.kv`,this.appId=r,this.createTopicOptions=e,this.store=new O(`db/${r}`,{cache:!0,...o}),this.store.on("change",u=>this.handleStoreChange(u));let c=Array.isArray(s)?s.join(","):"localhost:19092";this.consumer=new k({"group.id":"kv","bootstrap.servers":c,"enable.auto.commit":!1,"enable.auto.offset.store":!1,...a?.global},{"auto.offset.reset":"beginning",...a?.topic||{}}),this.producer=new C({"bootstrap.servers":c,...i?.global},{...i?.topic}),this.producer.setPollInterval(10)}handleStoreChange(t){let{op:e,bucket:r,id:s,value:o,ttl:a}=t,i=[];e&&i.push({op:e}),r&&i.push({bucket:r}),a&&i.push({ttl:a});let c=e==="remove"?null:h.isBuffer(o)?o:h.from(o);try{this.producer.produce(this.topic,null,c,s,Date.now(),null,i)}catch(u){let g=u;if(l.ERRORS.ERR__QUEUE_FULL===g.code)console.log("Queue full, re-buffering message"),this.producer.poll();else throw u}}queryWatermark(t){return new Promise((e,r)=>{this.consumer.queryWatermarkOffsets(this.topic,t,5e3,(s,o)=>{if(s)return r(s);console.log("queryWatermarkOffsets",{partition:t,offsets:o}),this.watermarks.set(t,Number(o.highOffset)),this.topicOffsets.set(t,-1),e()})})}async setupPartitionStatus(t){await Promise.all(t.map(({partition:e})=>this.queryWatermark(e))),console.log("Partition status initialized:",this.watermarks)}async start(){this.consumer.setDefaultConsumeTimeout(1e3),this.consumer.connect(),this.producer.connect(),await Promise.all([p(this.consumer,"ready"),p(this.producer,"ready")]),w.createFrom(this.consumer).createTopic({num_partitions:5,replication_factor:3,config:{"segment.ms":"300000","segment.bytes":"102400","cleanup.policy":"compact"},...this.createTopicOptions,topic:this.topic},async e=>{if(e&&e.code!==l.ERRORS.ERR_TOPIC_ALREADY_EXISTS)throw e;let r=await m(this.consumer,{}),s=f(r,[this.topic]);this.consumer.assign(s),await this.setupPartitionStatus(s),this.consumer.consume(),this.consumer.on("data",o=>this.handleConsumerData(o))}),await this.waitUntilCaughtUp()}handleConsumerData(t){this.topicOffsets.set(t.partition,t.offset);let e=t.headers?.reduce((r,s)=>{let o=Object.keys(s)[0];return r[o]=s[o].toString(),r},{});if(t.key&&e?.bucket&&e?.op){let r=this.store.bucket(e.bucket,{encoding:"json"});e.op==="put"?r.put(t.key.toString(),t.value,{quiet:!0}):r.remove(t.key.toString(),{quiet:!0})}}waitUntilCaughtUp(){return new Promise(t=>{let e=setInterval(()=>{this.hasCaughtUp()&&(clearInterval(e),console.log("All partitions caught up. KV is ready."),this.initialized=!0,t())},1e3)})}hasCaughtUp(){return Array.from(this.watermarks.entries()).every(([t,e])=>{let r=this.topicOffsets.get(t);return r!==void 0&&r>=e-1})}async stop(){this.consumer.unsubscribe(),this.consumer.pause(this.consumer.assignments()),this.producer.flush(5e3,t=>{console.log(t),this.producer.disconnect(),this.consumer.disconnect()}),await Promise.all([this.consumer,this.producer].map(t=>p(t,"disconnected")))}};export{d as Bucketsd};
//# sourceMappingURL=index.mjs.map